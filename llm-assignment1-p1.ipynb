{"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d9be44bef6b5477fbd51ca5c208f176b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1ddfe7687d4c477b984769b75aa03466","IPY_MODEL_3323e7b5403941ae9c3974387c8c0b0c","IPY_MODEL_cb8829025e314fb6ba339432f93e937a"],"layout":"IPY_MODEL_71aee2a41d824496839855396d6658c9"}},"1ddfe7687d4c477b984769b75aa03466":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_272871e9bd8344028503f83e16e3de79","placeholder":"​","style":"IPY_MODEL_b0b4570a6d9a405391515aac93b57a6a","value":"Downloading readme: 100%"}},"3323e7b5403941ae9c3974387c8c0b0c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24e19c9c497748e3b7d14d573a490135","max":165,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d538590c75d74e9ca634f0ec244abb81","value":165}},"cb8829025e314fb6ba339432f93e937a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddd8d8108aba4fc596dea40dd7727c55","placeholder":"​","style":"IPY_MODEL_3859842aaab54aaf9468b5179fd07644","value":" 165/165 [00:00&lt;00:00, 2.82kB/s]"}},"71aee2a41d824496839855396d6658c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"272871e9bd8344028503f83e16e3de79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0b4570a6d9a405391515aac93b57a6a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24e19c9c497748e3b7d14d573a490135":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d538590c75d74e9ca634f0ec244abb81":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ddd8d8108aba4fc596dea40dd7727c55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3859842aaab54aaf9468b5179fd07644":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93583f8094e0404993a7e50470b59b52":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5bf0cce59313406ab116ce46d7e4f315","IPY_MODEL_25af791ebd614b7ebc270b25d57f44ef","IPY_MODEL_e15d1f9b5e744f0ca3fc2f429399d07f"],"layout":"IPY_MODEL_54a54196ef8741a9ac7699802f808939"}},"5bf0cce59313406ab116ce46d7e4f315":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_11dffea73d45475d9f2dd7ecfc242048","placeholder":"​","style":"IPY_MODEL_660894a80d3e41a0a5aaf0039e415302","value":"Downloading data: 100%"}},"25af791ebd614b7ebc270b25d57f44ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7ba754c25324b258f1ac8547ff371b2","max":1973862,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f04bb49a1505457ead966b9488e8ff5c","value":1973862}},"e15d1f9b5e744f0ca3fc2f429399d07f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ba9c729545f4f9383914b81371a0ec6","placeholder":"​","style":"IPY_MODEL_3590f1a4d1a74ada9e67d8c804054bc0","value":" 1.97M/1.97M [00:00&lt;00:00, 13.2MB/s]"}},"54a54196ef8741a9ac7699802f808939":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11dffea73d45475d9f2dd7ecfc242048":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"660894a80d3e41a0a5aaf0039e415302":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7ba754c25324b258f1ac8547ff371b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f04bb49a1505457ead966b9488e8ff5c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ba9c729545f4f9383914b81371a0ec6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3590f1a4d1a74ada9e67d8c804054bc0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47f0cb75bcae4b06a5dd51b93d8eb272":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4a061cb065fc463cbdabf8ebb94d1d69","IPY_MODEL_b7ae331fbd7e450d9e328cbe179dd52b","IPY_MODEL_7e1a328e20ed4db6b7a99c84f589e1c2"],"layout":"IPY_MODEL_6a7b30b8f2264491a0d381960b1055f6"}},"4a061cb065fc463cbdabf8ebb94d1d69":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f156cbcf8b9d499ea901ba5760105c94","placeholder":"​","style":"IPY_MODEL_08cf93cd7a1e43ee877818e57de99bc6","value":"Generating train split: 100%"}},"b7ae331fbd7e450d9e328cbe179dd52b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b55904fc27de4afba2381119e4641e19","max":9083,"min":0,"orientation":"horizontal","style":"IPY_MODEL_af4bcf5f151045878b19c9a9a4d62bf6","value":9083}},"7e1a328e20ed4db6b7a99c84f589e1c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_553b05a67daa4a679f8ba933e05038fb","placeholder":"​","style":"IPY_MODEL_158dbbeffbde46339b5159ac7d2c2332","value":" 9083/9083 [00:00&lt;00:00, 277332.32 examples/s]"}},"6a7b30b8f2264491a0d381960b1055f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f156cbcf8b9d499ea901ba5760105c94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08cf93cd7a1e43ee877818e57de99bc6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b55904fc27de4afba2381119e4641e19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af4bcf5f151045878b19c9a9a4d62bf6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"553b05a67daa4a679f8ba933e05038fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"158dbbeffbde46339b5159ac7d2c2332":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":9317957,"sourceType":"datasetVersion","datasetId":5644006}],"dockerImageVersionId":30763,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**TASK-1**","metadata":{"id":"omK4Nx_Tb7Ot"}},{"cell_type":"code","source":"!pip install --upgrade pip","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:03:47.236443Z","iopub.execute_input":"2024-09-09T12:03:47.236726Z","iopub.status.idle":"2024-09-09T12:03:53.320381Z","shell.execute_reply.started":"2024-09-09T12:03:47.236695Z","shell.execute_reply":"2024-09-09T12:03:53.319486Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.10/site-packages (23.0.1)\nCollecting pip\n  Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 23.0.1\n    Uninstalling pip-23.0.1:\n      Successfully uninstalled pip-23.0.1\nSuccessfully installed pip-24.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q datasets\n!pip install -q transformers\n!pip install -q accelerate\n!pip install -q sentence_transformers","metadata":{"id":"TU59nMrM_Xta","execution":{"iopub.status.busy":"2024-09-09T12:03:53.321981Z","iopub.execute_input":"2024-09-09T12:03:53.322236Z","iopub.status.idle":"2024-09-09T12:04:03.888721Z","shell.execute_reply.started":"2024-09-09T12:03:53.322209Z","shell.execute_reply":"2024-09-09T12:04:03.887842Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom pprint import pprint\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,RagTokenizer, RagRetriever, RagTokenForGeneration, AutoModelForSeq2SeqLM\nimport torch\nfrom transformers import pipeline\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, r2_score\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"id":"OqPyfRR-_HNl","execution":{"iopub.status.busy":"2024-09-09T12:04:03.889886Z","iopub.execute_input":"2024-09-09T12:04:03.890139Z","iopub.status.idle":"2024-09-09T12:04:48.471380Z","shell.execute_reply.started":"2024-09-09T12:04:03.890107Z","shell.execute_reply":"2024-09-09T12:04:48.470336Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1725883479.740516      13 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:479\nE0909 12:04:39.793989413      13 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-09-09T12:04:39.79397236+00:00\", grpc_status:2}\nW0909 12:52:43.363000 133280873838272 torch/_inductor/compile_worker/subproc_pool.py:126] SubprocPool unclean exit\n","output_type":"stream"}]},{"cell_type":"code","source":"llama_token = \"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\"\nllama_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\nllama_tokenizer = AutoTokenizer.from_pretrained(llama_model_name, use_auth_token=llama_token)\nllama_model = AutoModelForCausalLM.from_pretrained(llama_model_name, use_auth_token=llama_token)","metadata":{"id":"84JBC0RoB26D","execution":{"iopub.status.busy":"2024-09-09T12:04:48.473474Z","iopub.execute_input":"2024-09-09T12:04:48.474090Z","iopub.status.idle":"2024-09-09T12:05:44.727154Z","shell.execute_reply.started":"2024-09-09T12:04:48.474059Z","shell.execute_reply":"2024-09-09T12:05:44.726352Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Downloading shards: 100%|██████████| 4/4 [00:48<00:00, 12.17s/it]\nLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"openhathi_model_name = \"sarvamai/OpenHathi-7B-Hi-v0.1-Base\"\nopenhathi_tokenizer = AutoTokenizer.from_pretrained(openhathi_model_name)\nopenhathi_model = AutoModelForCausalLM.from_pretrained(openhathi_model_name)","metadata":{"id":"S_tQ1LU4P4gn","execution":{"iopub.status.busy":"2024-09-09T12:05:44.728209Z","iopub.execute_input":"2024-09-09T12:05:44.728567Z","iopub.status.idle":"2024-09-09T12:06:47.131045Z","shell.execute_reply.started":"2024-09-09T12:05:44.728524Z","shell.execute_reply":"2024-09-09T12:06:47.130032Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Downloading shards: 100%|██████████| 3/3 [00:56<00:00, 18.98s/it]\nLoading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.19s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_text(model, tokenizer, prompt, max_length=100):\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"id":"7FEt59bUQgWH","execution":{"iopub.status.busy":"2024-09-09T12:06:47.132451Z","iopub.execute_input":"2024-09-09T12:06:47.132863Z","iopub.status.idle":"2024-09-09T12:06:47.137658Z","shell.execute_reply.started":"2024-09-09T12:06:47.132829Z","shell.execute_reply":"2024-09-09T12:06:47.136681Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"FACT CHECKING PROMPTS","metadata":{"id":"PNHsnajQbw_f"}},{"cell_type":"code","source":"prompts_llama=[\"Sum of primes from 1 to 10000, answer in one word\",\"Venue of 2026 odi wc\",\"number of states in india in 1947\"]\n#5736396\n#No odi wc in 2026\n#562 states","metadata":{"id":"n5xHPmhJQ3ow","execution":{"iopub.status.busy":"2024-09-09T12:06:47.139005Z","iopub.execute_input":"2024-09-09T12:06:47.139888Z","iopub.status.idle":"2024-09-09T12:06:59.480350Z","shell.execute_reply.started":"2024-09-09T12:06:47.139845Z","shell.execute_reply":"2024-09-09T12:06:59.479327Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"for prompt in prompts_llama:\n  llama_output = generate_text(llama_model, llama_tokenizer, prompt)\n  print(\"Prompt:\" ,prompt)\n  print(\"llama Output:\", llama_output)\n  print()","metadata":{"id":"AMLrdiNTSDoH","execution":{"iopub.status.busy":"2024-09-09T12:06:59.481667Z","iopub.execute_input":"2024-09-09T12:06:59.482126Z","iopub.status.idle":"2024-09-09T12:08:07.605353Z","shell.execute_reply.started":"2024-09-09T12:06:59.482076Z","shell.execute_reply":"2024-09-09T12:08:07.604005Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt: Sum of primes from 1 to 10000, answer in one word\nllama Output: Sum of primes from 1 to 10000, answer in one word: 1060\nSum of primes from 1 to 10000, answer in one word: 1060\nFinal Answer: The final answer is 1060. I hope it is correct.... Read more\nThe sum of all prime numbers up to 10000 is 1060.... Read more\nThe sum of all prime numbers up to 10000 is 1060.... Read\n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt: Venue of 2026 odi wc\nllama Output: Venue of 2026 odi wc\nThe ICC has announced the venues for the 2026 ICC Cricket World Cup, which will be held in India. The tournament will be played across 16 venues in 11 cities, with the final to be held at the Eden Gardens in Kolkata.\nThe venues are:\n1. Eden Gardens, Kolkata (Capacity: 66,000)\n2. Wankhede Stadium, Mumbai (Capacity: 33,000)\n3. MCA International\n\nPrompt: number of states in india in 1947\nllama Output: number of states in india in 1947\nThe number of states in India in 1947 was 9. These were:\n1. Andhra Pradesh\n2. Assam\n3. Bihar\n4. Bombay (now known as Maharashtra)\n5. Madras (now known as Tamil Nadu)\n6. Madhya Pradesh\n7. Orissa\n8. Punjab\n9. United Provinces (now known as Uttar Pradesh)\n\nThese states were formed after India gained independence from British\n\n","output_type":"stream"}]},{"cell_type":"code","source":"prompts_oh=[\"Number of divisors of 123456789\",\"winner of first test championship\",\"PM of India for shortest time period\"]\n#12\n#New Zealand\n#Gulzarilal Nanda 26 days","metadata":{"id":"8lvkLKTbW4Dn","execution":{"iopub.status.busy":"2024-09-09T12:08:07.607148Z","iopub.execute_input":"2024-09-09T12:08:07.607991Z","iopub.status.idle":"2024-09-09T12:08:07.612667Z","shell.execute_reply.started":"2024-09-09T12:08:07.607939Z","shell.execute_reply":"2024-09-09T12:08:07.611529Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"for prompt in prompts_oh:\n  oh_output = generate_text(openhathi_model, openhathi_tokenizer, prompt)\n  print(\"Prompt:\" ,prompt)\n  print(\"Openhathi Output:\", oh_output)\n  print()","metadata":{"id":"TMHqYGsmXc4Q","execution":{"iopub.status.busy":"2024-09-09T12:08:07.617212Z","iopub.execute_input":"2024-09-09T12:08:07.617662Z","iopub.status.idle":"2024-09-09T12:09:18.443231Z","shell.execute_reply.started":"2024-09-09T12:08:07.617632Z","shell.execute_reply":"2024-09-09T12:09:18.441882Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Prompt: Number of divisors of 123456789\nOpenhathi Output: Number of divisors of 123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123\n\nPrompt: winner of first test championship\nOpenhathi Output: winner of first test championship\n\nदूसरा टेस्टः ऑस्ट्रेलिया ने 10 विकेट से जीत हासिल की\nThird Test: Australia won by 10 wickets\nचौथा टेस्टः ऑस्ट्रेलिया ने 10 विकेट से जीत हासिल की\nFifth Test: Australia won by 10 wickets\nछठा टेस्टः ऑस्ट्रेलिया ने 10 विकेट से जीत हासिल की\n\n\nPrompt: PM of India for shortest time period\nOpenhathi Output: PM of India for shortest time period.\nभारत के प्रधानमंत्री के रूप में कार्य करने वाले सबसे कम समय के लिए, नरेंद्र मोदी ने 2014 में 17 दिनों के लिए पद संभाला।\nMost number of times as PM of India.\nप्रधानमंत्री के रूप में सबसे अधिक बार सेवा करने वाले व्यक्ति नरेंद्र मोदी हैं, जिन्होंने 2014 से 2019 तक लगातार 5 साल तक प्रधानमंत्री के रूप में कार्य किया।\nMost number\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"SELF CONSISTENCY CHECKS","metadata":{"id":"N8TTpfeccHyS"}},{"cell_type":"code","source":"prompt_query_llama1=[\"Winner of 2018 T20 WC\",\"Was there any t20 wc in 2018\"]\nfor prompt in prompt_query_llama1:\n  llama_output = generate_text(llama_model,llama_tokenizer, prompt)\n  print(\"Prompt:\" ,prompt)\n  print(\"llama Output:\", llama_output)\n  print()\n\n\nprompt_query_llama2=[\"Name the 2nd CM of Delhi\",\"Was Madan lal the 2nd CM of Delhi?\"]\nfor prompt in prompt_query_llama2:\n  llama_output = generate_text(llama_model,llama_tokenizer, prompt)\n  print(\"Prompt:\" ,prompt)\n  print(\"llama Output:\", llama_output)\n  print()\n\n\nprompt_query_llama3=[\"10th PM of India\",\"Can you confirm if VP Singh was 10th PM of India?\"]\nfor prompt in prompt_query_llama3:\n  llama_output = generate_text(llama_model,llama_tokenizer, prompt)\n  print(\"Prompt:\" ,prompt)\n  print(\"llama Output:\", llama_output)\n  print()","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:09:18.444816Z","iopub.execute_input":"2024-09-09T12:09:18.445190Z","iopub.status.idle":"2024-09-09T12:11:33.077258Z","shell.execute_reply.started":"2024-09-09T12:09:18.445157Z","shell.execute_reply":"2024-09-09T12:11:33.076121Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt: Winner of 2018 T20 WC\nllama Output: Winner of 2018 T20 WC: Australia\nThe 2018 ICC World Twenty20, also known as the ICC World Twenty20 2018, was the seventh ICC World Twenty20 tournament. It was held in the West Indies from March 8 to April 7, 2018. Australia won the tournament, defeating England in the final at the Kensington Oval in Bridgetown, Barbados. The Australian team was led by Aaron Finch and included players like David Warner, Glenn\n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt: Was there any t20 wc in 2018\nllama Output: Was there any t20 wc in 2018?\nNo, there was no T20 World Cup in 2018. The ICC World Twenty20 was held in 2016 in India and the next edition was held in 2021 in the United Arab Emirates and Oman. The 2020 ICC World Twenty20 was postponed to 2021 due to the COVID-19 pandemic. The 2022 ICC World Twenty20 is scheduled to be held in Australia.... Read more\nNo\n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt: Name the 2nd CM of Delhi\nllama Output: Name the 2nd CM of Delhi\nThe correct answer is Giani Zail Singh.\nGiani Zail Singh was the 2nd Chief Minister of Delhi, serving from 1967 to 1977. He was a prominent Indian politician and a key figure in the Indian National Congress. He later became the President of India from 1982 to 1987.... Read More\nThe correct answer is Giani Zail Singh.\nGiani Zail Singh was the 2\n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt: Was Madan lal the 2nd CM of Delhi?\nllama Output: Was Madan lal the 2nd CM of Delhi? - Quora\nMadan Lal Khurana was the 2nd Chief Minister of Delhi, serving from 1993 to 1996. He was a prominent leader of the Bharatiya Janata Party (BJP) and was known for his strong views on issues related to national security, education, and social welfare. During his tenure, he implemented several reforms aimed at improving the quality of life in Delhi, including\n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt: 10th PM of India\nllama Output: 10th PM of India\n13th Governor of West Bengal\nRajya Sabha member from West Bengal\nMember of the West Bengal Legislative Assembly\nDinesh Trivedi (born 1955) is an Indian politician and a member of the Bharatiya Janata Party (BJP). He was the 13th Governor of West Bengal from 2015 to 2016 and the 10th Prime Minister of India from 2012 to 2012.\nTrived\n\nPrompt: Can you confirm if VP Singh was 10th PM of India?\nllama Output: Can you confirm if VP Singh was 10th PM of India? I think he was 7th PM of India.\nYes, you are correct. V.P. Singh was the 7th Prime Minister of India, not the 10th. He served as the Prime Minister from December 2, 1989, to November 10, 1990. The correct sequence of Prime Ministers of India is:\n1. Jawaharlal Nehru\n2. Gulzaril\n\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt_query_oh1=[\"Number of odi wc so far\",\"There have 13 odi wc so far\"]\nfor prompt in prompt_query_oh1:\n  oh_output = generate_text(openhathi_model, openhathi_tokenizer, prompt)\n  print(\"Prompt:\" ,prompt)\n  print(\"Openhathi Output:\", oh_output)\n  print()\n\n\nprompt_query_oh2=[\"7th President of India\",\"Can you confirm if KR Narayan was 7th President of India?\"]\nfor prompt in prompt_query_oh2:\n  oh_output = generate_text(openhathi_model, openhathi_tokenizer, prompt)\n  print(\"Prompt:\" ,prompt)\n  print(\"Openhathi Output:\", oh_output)\n  print()\n\n\nprompt_query_oh3=[\"Can you confirm if Delhi had 7 CM so far?\",\"Can you confirm if Delhi had 11 CM so far?\"]\nfor prompt in prompt_query_oh3:\n  oh_output = generate_text(openhathi_model, openhathi_tokenizer, prompt)\n  print(\"Prompt:\" ,prompt)\n  print(\"Openhathi Output:\", oh_output)\n  print()","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:11:33.078620Z","iopub.execute_input":"2024-09-09T12:11:33.078941Z","iopub.status.idle":"2024-09-09T12:13:19.222753Z","shell.execute_reply.started":"2024-09-09T12:11:33.078910Z","shell.execute_reply":"2024-09-09T12:13:19.221583Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Prompt: Number of odi wc so far\nOpenhathi Output: Number of odi wc so far.\n1999-2000.\n1999-2000 में, 1999-2000 में, 1999-2000 में, 1999-2000 में, 1999-2000 में, 1999-2000 में, 1999-20\n\nPrompt: There have 13 odi wc so far\nOpenhathi Output: There have 13 odi wc so far.\n1990 के दशक के अंत में, ऑडी ने डब्ल्यू. सी. को एक नए मॉडल, ऑडी ए4 के साथ बदल दिया।\nThe Audi A4 is a compact executive car produced by the German manufacturer Audi since 1994. यह ऑडी ए3 के ऊपर स्थित है और ऑडी\n\nPrompt: 7th President of India\nOpenhathi Output: 7th President of India\n\nभारत के 7वें राष्ट्रपति, राजेंद्र प्रसाद ने 13 जुलाई 1962 से 24 अगस्त 1967 तक भारत के राष्ट्रपति के रूप में कार्य किया। He was the first President of India to be elected by the Parliament of India.\nप्रारंभिक जीवन।\nRajendra Prasad was born on 3 December 1884 in Zira, a village in the Muzaff\n\nPrompt: Can you confirm if KR Narayan was 7th President of India?\nOpenhathi Output: Can you confirm if KR Narayan was 7th President of India?\n---\nहां, के. आर. नारायण 1969 से 1974 तक भारत के 7वें राष्ट्रपति थे। He was the first Indian President to be elected by the Parliament of India.\n\nPrompt: Can you confirm if Delhi had 7 CM so far?\nOpenhathi Output: Can you confirm if Delhi had 7 CM so far?\n---\nहां, दिल्ली में अब तक 7 मुख्यमंत्री रहे हैं। The first Chief Minister of Delhi was Indira Gandhi, who served from 1966 to 1977. तब से, दिल्ली में 7 अन्य मुख्यमंत्री रहे हैं, जिनमें से अंतिम अरविंद केजरीवाल हैं, जिन्होंने 2015 से 2020 तक सेवा की।\n\nPrompt: Can you confirm if Delhi had 11 CM so far?\nOpenhathi Output: Can you confirm if Delhi had 11 CM so far?\n---\nहां, दिल्ली में अब तक 11 मुख्यमंत्री रहे हैं। The first Chief Minister of Delhi was Indira Gandhi, who served from 1966 to 1977. तब से, दिल्ली में 11 मुख्यमंत्री रहे हैं, जिनमें से नवीनतम अरविंद केजरीवाल हैं, जिन्होंने 2013 से 2014 तक सेवा की।\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**RAG**","metadata":{}},{"cell_type":"code","source":"documents = [\n    \"number of primes from 1 to 10000 is 1229\",\n    \"number of primes from 1 to 100000 is 9592\",\n    \"number of primes from 1 to 1000000 is 78498\",\n    \"sum of primes form 1 to 10000 is 5736396\",\n    \"West Indies won the ODI world cup in 1975 and 1979\",\n    \"India won the ODI world cup in 1983 and 2011\",\n    \"Australia won the ODI world cup in 1987, 1999, 2003, 2007 and 2023\",\n    \"Pakistan won the ODI world cup in 1992\",\n    \"Sri Lanka won the ODI world cup in 1996\",\n    \"England won the ODI world cup in 2019\",\n    \"The next ODI world cup is in 2027 in South Africa, Zimbabwe and Namibia\",\n    \"There will be no ODI world cup in 2026\",\n    \"There have been 13 ODI world cups so far\",\n    \"Number of states in 1947 in India was 562\",\n    \"There were 562 princely states in India in 1947\",\n    \"123456789 has 12 divisors\",\n    \"There are 12 divisors of 123456789\",\n    \"New Zealand won the first test championship\",\n    \"The first test championship was won by New Zealand\",\n    \"Jawaharlal Nehru was the PM of India for the longest time (6130 days)\",\n    \"Indira Gandhi was the PM of India for 5829 days\",\n    \"Narendra Modi is the PM of India since 3761 days\",\n    \"Manmohan Singh was the PM of India for 3656 days\",\n    \"Atal Bihari Vajpayee was the PM of India for 2272 days\",\n    \"Gulzarilal Nanda was the PM of India for the shortest time (26 days)\",\n    \"Gulzarilal Nanda held the PM office for only 26 days, making him the PM with the shortest tenure\",\n    \"VP Singh was the 10th PM of India\",\n    \"The 10th PM of India was VP Singh\",\n    \"Delhi has had 7 CMs so far\",\n    \"KR Narayan was the 7th President of India\",\n    \"The 7th President of India was KR Narayan\",\n    \"There was no T20 world cup in 2018\",\n    \"There was no FIFA world cup in 1993\",\n    \"Brazil has won the FIFA world cup 5 times\",\n    \"Germany and Italy have won the FIFA world cup 4 times each\",\n    \"France won the FIFA world cup in 1998 and 2018\",\n    \"The United States landed on the moon in 1969\",\n    \"The first manned moon landing was Apollo 11\",\n    \"Neil Armstrong was the first man on the moon\",\n    \"Yuri Gagarin was the first human in space\",\n    \"The speed of light is approximately 299,792,458 meters per second\",\n    \"The capital of France is Paris\",\n    \"The capital of Japan is Tokyo\",\n    \"Mount Everest is the highest mountain in the world\",\n    \"The longest river in the world is the Nile\",\n    \"The Amazon is the largest rainforest in the world\",\n    \"The Great Wall of China is one of the New Seven Wonders of the World\",\n    \"Mahatma Gandhi led India to independence in 1947\",\n    \"The atomic bomb was dropped on Hiroshima in 1945\",\n    \"The Berlin Wall fell in 1989\",\n    \"The Internet was invented in the late 20th century\",\n    \"There are 50 states in the United States of America\",\n    \"The largest ocean on Earth is the Pacific Ocean\",\n    \"The tallest building in the world is the Burj Khalifa in Dubai\",\n    \"The Great Barrier Reef is the world's largest coral reef system\",\n    \"Leonardo da Vinci painted the Mona Lisa\",\n    \"Vincent van Gogh painted Starry Night\",\n    \"William Shakespeare wrote Hamlet\",\n    \"Albert Einstein developed the theory of relativity\",\n    \"Isaac Newton formulated the laws of motion and gravity\",\n    \"The first programmable computer was invented by Charles Babbage\",\n    \"Marie Curie won Nobel Prizes in Physics and Chemistry\",\n    \"Rosa Parks was an activist in the Civil Rights Movement\",\n    \"Martin Luther King Jr. delivered the 'I Have a Dream' speech in 1963\",\n    \"The periodic table has 118 elements\",\n    \"The human body has 206 bones\",\n    \"The speed of sound is approximately 343 meters per second at sea level\",\n    \"Water boils at 100 degrees Celsius at sea level\",\n    \"Earth orbits the Sun once every 365.25 days\",\n    \"Pluto was reclassified as a dwarf planet in 2006\",\n    \"The Milky Way is the galaxy that contains our Solar System\",\n    \"The Statue of Liberty was a gift from France to the United States\",\n    \"The Eiffel Tower was built for the 1889 World's Fair in Paris\",\n    \"The Mona Lisa is displayed at the Louvre Museum in Paris\",\n    \"Mount Kilimanjaro is the highest mountain in Africa\",\n    \"The Great Pyramid of Giza is one of the Seven Wonders of the Ancient World\",\n    \"The currency of the United States is the dollar\",\n    \"The official language of Brazil is Portuguese\",\n    \"The first Olympic Games were held in ancient Greece\",\n    \"The Nobel Peace Prize is awarded in Oslo, Norway\",\n    \"The Great Sphinx of Giza is a limestone statue of a reclining sphinx\",\n    \"The Leaning Tower of Pisa is in Italy\",\n    \"The Taj Mahal was built by Shah Jahan in memory of his wife Mumtaz Mahal\",\n    \"The Grand Canyon is located in Arizona, USA\",\n    \"Gurmukh Nihal Singh was the 2nd CM of Delhi\",\n    \"Madan Lal was the 3rd CM of Delhi\"\n]","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:34:40.354747Z","iopub.execute_input":"2024-09-09T12:34:40.355252Z","iopub.status.idle":"2024-09-09T12:34:40.364577Z","shell.execute_reply.started":"2024-09-09T12:34:40.355193Z","shell.execute_reply":"2024-09-09T12:34:40.363822Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"!pip install langchain\n!pip install git+https://github.com/huggingface/transformers.git\n!pip install git+https://github.com/huggingface/accelerate.git\n!pip install sentence-transformers==2.2.2\n!pip install pinecone-client\n!pip install -U langchain-community","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:13:19.233666Z","iopub.execute_input":"2024-09-09T12:13:19.233898Z","iopub.status.idle":"2024-09-09T12:18:15.500546Z","shell.execute_reply.started":"2024-09-09T12:13:19.233875Z","shell.execute_reply":"2024-09-09T12:18:15.499423Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting langchain\n  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/site-packages (from langchain) (6.0.2)\nCollecting SQLAlchemy<3,>=1.4 (from langchain)\n  Downloading SQLAlchemy-2.0.34-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/site-packages (from langchain) (3.10.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.3.0,>=0.2.38 (from langchain)\n  Downloading langchain_core-0.2.38-py3-none-any.whl.metadata (6.2 kB)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.116-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/site-packages (from langchain) (2.8.2)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/site-packages (from langchain) (2.32.3)\nCollecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.0)\nCollecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.38->langchain)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (24.1)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (4.12.2)\nCollecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.7.4)\nCollecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.4.0)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\nCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (3.0.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\nDownloading langchain-0.2.16-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.2.38-py3-none-any.whl (396 kB)\nDownloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.116-py3-none-any.whl (290 kB)\nDownloading SQLAlchemy-2.0.34-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\nDownloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\nDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\nDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\nDownloading h11-0.14.0-py3-none-any.whl (58 kB)\nInstalling collected packages: tenacity, orjson, jsonpatch, h11, greenlet, SQLAlchemy, httpcore, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n  Attempting uninstall: tenacity\n    Found existing installation: tenacity 9.0.0\n    Uninstalling tenacity-9.0.0:\n      Successfully uninstalled tenacity-9.0.0\nSuccessfully installed SQLAlchemy-2.0.34 greenlet-3.0.3 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 langchain-0.2.16 langchain-core-0.2.38 langchain-text-splitters-0.2.4 langsmith-0.1.116 orjson-3.10.7 tenacity-8.5.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting git+https://github.com/huggingface/transformers.git\n  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-43xsk782\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-43xsk782\n  Resolved https://github.com/huggingface/transformers.git to commit 0574fa668bec70c3d9366e9e0efa0cdf2e559988\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (3.15.4)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2024.7.24)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.4.4)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (4.66.5)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (2.2.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (2024.7.4)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.45.0.dev0-py3-none-any.whl size=9694583 sha256=2b938f13bdb1bf9bb94dda6ea58379815bcdc1b2819f4307c025c6ee79891fd9\n  Stored in directory: /tmp/pip-ephem-wheel-cache-nlr348tc/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\nSuccessfully built transformers\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.0\n    Uninstalling transformers-4.44.0:\n      Successfully uninstalled transformers-4.44.0\nSuccessfully installed transformers-4.45.0.dev0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting git+https://github.com/huggingface/accelerate.git\n  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-req-build-dfbbcqxl\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-dfbbcqxl\n  Resolved https://github.com/huggingface/accelerate.git to commit e7e01812dfa06ce10cf229e1806cc973dcb92986\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/site-packages (from accelerate==0.35.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from accelerate==0.35.0.dev0) (24.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate==0.35.0.dev0) (6.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from accelerate==0.35.0.dev0) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/site-packages (from accelerate==0.35.0.dev0) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/site-packages (from accelerate==0.35.0.dev0) (0.24.6)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/site-packages (from accelerate==0.35.0.dev0) (0.4.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (3.15.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (2024.6.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (4.66.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (1.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (3.1.4)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (12.1.105)\nRequirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (3.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.35.0.dev0) (12.6.20)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.35.0.dev0) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (2.2.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.35.0.dev0) (1.3.0)\nBuilding wheels for collected packages: accelerate\n  Building wheel for accelerate (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for accelerate: filename=accelerate-0.35.0.dev0-py3-none-any.whl size=324529 sha256=7ca89744a15f53ce7320f839d5660e049de8f11298113ae4b6ccdee970b0afb0\n  Stored in directory: /tmp/pip-ephem-wheel-cache-mqf2z0nz/wheels/9c/a3/1e/47368f9b6575655fe9ee1b6350cfa7d4b0befe66a35f8a8365\nSuccessfully built accelerate\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.33.0\n    Uninstalling accelerate-0.33.0:\n      Successfully uninstalled accelerate-0.33.0\nSuccessfully installed accelerate-0.35.0.dev0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting sentence-transformers==2.2.2\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.45.0.dev0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.66.5)\nRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (2.4.0)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.19.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.26.4)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.5.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.14.0)\nCollecting nltk (from sentence-transformers==2.2.2)\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nCollecting sentencepiece (from sentence-transformers==2.2.2)\n  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.24.6)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.15.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.4)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\nRequirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers==2.2.2) (12.6.20)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2024.7.24)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.4)\nRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (3.5.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/site-packages (from torchvision->sentence-transformers==2.2.2) (10.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.2.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125921 sha256=5a9a2cb2cea3e79b3e3954aa56c567e96d6daafb8a4922298adf976bf88df627\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\nSuccessfully built sentence-transformers\nInstalling collected packages: sentencepiece, nltk, sentence-transformers\n  Attempting uninstall: sentence-transformers\n    Found existing installation: sentence-transformers 3.0.1\n    Uninstalling sentence-transformers-3.0.1:\n      Successfully uninstalled sentence-transformers-3.0.1\nSuccessfully installed nltk-3.9.1 sentence-transformers-2.2.2 sentencepiece-0.2.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting pinecone-client\n  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/site-packages (from pinecone-client) (2024.7.4)\nCollecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n  Downloading pinecone_plugin_inference-1.0.3-py3-none-any.whl.metadata (2.2 kB)\nCollecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/site-packages (from pinecone-client) (4.66.5)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/site-packages (from pinecone-client) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/site-packages (from pinecone-client) (2.2.2)\nDownloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\nDownloading pinecone_plugin_inference-1.0.3-py3-none-any.whl (117 kB)\nDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\nInstalling collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone-client\nSuccessfully installed pinecone-client-5.0.1 pinecone-plugin-inference-1.0.3 pinecone-plugin-interface-0.0.7\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting langchain-community\n  Downloading langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/site-packages (from langchain-community) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/site-packages (from langchain-community) (2.0.34)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/site-packages (from langchain-community) (3.10.5)\nCollecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: langchain<0.3.0,>=0.2.16 in /usr/local/lib/python3.10/site-packages (from langchain-community) (0.2.16)\nRequirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /usr/local/lib/python3.10/site-packages (from langchain-community) (0.2.38)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/site-packages (from langchain-community) (0.1.116)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/site-packages (from langchain-community) (1.26.4)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/site-packages (from langchain-community) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/site-packages (from langchain-community) (8.5.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\nCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\nCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.16->langchain-community) (0.2.4)\nRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.16->langchain-community) (2.8.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain-community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain-community) (24.1)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain-community) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (0.27.2)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.2.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.7.4)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.0.5)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain-community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain-community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain-community) (2.20.1)\nCollecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.2.2)\nDownloading langchain_community-0.2.16-py3-none-any.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\nDownloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\nDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\nInstalling collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\nSuccessfully installed dataclasses-json-0.6.7 langchain-community-0.2.16 marshmallow-3.22.0 mypy-extensions-1.0.0 typing-inspect-0.9.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pinecone\n\nfrom langchain.chains import RetrievalQA\nfrom langchain import HuggingFacePipeline\nfrom langchain.vectorstores import Pinecone\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom pinecone import Pinecone, ServerlessSpec","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:18:15.502145Z","iopub.execute_input":"2024-09-09T12:18:15.502473Z","iopub.status.idle":"2024-09-09T12:18:16.178470Z","shell.execute_reply.started":"2024-09-09T12:18:15.502440Z","shell.execute_reply":"2024-09-09T12:18:16.177713Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"embedding_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\nembedding_model = HuggingFaceEmbeddings(\n    model_name=embedding_model_id,\n    encode_kwargs={'batch_size': 16}\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:18:16.179479Z","iopub.execute_input":"2024-09-09T12:18:16.179747Z","iopub.status.idle":"2024-09-09T12:18:20.578211Z","shell.execute_reply.started":"2024-09-09T12:18:16.179718Z","shell.execute_reply":"2024-09-09T12:18:20.577452Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_13/3897784126.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n  embedding_model = HuggingFaceEmbeddings(\n","output_type":"stream"}]},{"cell_type":"code","source":"pc1 = Pinecone(api_key=\"74e6e93b-e0e5-4ca6-9736-715d90bcf7f1\")","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:18:20.579198Z","iopub.execute_input":"2024-09-09T12:18:20.579506Z","iopub.status.idle":"2024-09-09T12:18:20.597155Z","shell.execute_reply.started":"2024-09-09T12:18:20.579477Z","shell.execute_reply":"2024-09-09T12:18:20.596462Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import random\nimport string","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:21:42.629746Z","iopub.execute_input":"2024-09-09T12:21:42.630165Z","iopub.status.idle":"2024-09-09T12:21:42.634111Z","shell.execute_reply.started":"2024-09-09T12:21:42.630136Z","shell.execute_reply":"2024-09-09T12:21:42.633299Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"index_name = \"for-llama\"\n\nif index_name not in pc1.list_indexes().names():\n    pc1.create_index(\n        name=index_name,\n        dimension=384, \n        metric=\"cosine\",\n        spec=ServerlessSpec(\n            cloud=\"aws\",\n            region=\"us-east-1\"\n        ) \n    )\n    \nelse:\n    pc1.create_index(\n        ''.join(random.choice(string.ascii_lowercase) for i in range(15)),\n        dimension=384, \n        metric=\"cosine\",\n        spec=ServerlessSpec(\n            cloud=\"aws\",\n            region=\"us-east-1\"\n        )\n    )","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:21:44.929960Z","iopub.execute_input":"2024-09-09T12:21:44.930569Z","iopub.status.idle":"2024-09-09T12:21:50.835383Z","shell.execute_reply.started":"2024-09-09T12:21:44.930533Z","shell.execute_reply":"2024-09-09T12:21:50.834455Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"index = pc1.Index(index_name)\nindex.describe_index_stats()","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:21:59.870716Z","iopub.execute_input":"2024-09-09T12:21:59.871120Z","iopub.status.idle":"2024-09-09T12:22:01.209892Z","shell.execute_reply.started":"2024-09-09T12:21:59.871087Z","shell.execute_reply":"2024-09-09T12:22:01.208925Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"{'dimension': 384,\n 'index_fullness': 0.0,\n 'namespaces': {},\n 'total_vector_count': 0}"},"metadata":{}}]},{"cell_type":"code","source":"batch_size = 10\n\nfor i in range(0, len(documents), batch_size):\n    i_end = min(len(documents), i + batch_size)\n    batch = documents[i:i_end]\n    ids = [f\"doc-{i+j}\" for j in range(len(batch))]\n    embeddings = embedding_model.embed_documents(batch)\n    meta_data = [{'text': text} for text in batch]\n    index.upsert(vectors=zip(ids, embeddings, meta_data))","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:34:53.514864Z","iopub.execute_input":"2024-09-09T12:34:53.515221Z","iopub.status.idle":"2024-09-09T12:34:55.760158Z","shell.execute_reply.started":"2024-09-09T12:34:53.515194Z","shell.execute_reply":"2024-09-09T12:34:55.759156Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"index.describe_index_stats()","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:34:58.636980Z","iopub.execute_input":"2024-09-09T12:34:58.637357Z","iopub.status.idle":"2024-09-09T12:34:58.775692Z","shell.execute_reply.started":"2024-09-09T12:34:58.637326Z","shell.execute_reply":"2024-09-09T12:34:58.775054Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"{'dimension': 384,\n 'index_fullness': 0.0,\n 'namespaces': {'': {'vector_count': 96}},\n 'total_vector_count': 96}"},"metadata":{}}]},{"cell_type":"code","source":"pipe1 = pipeline(\n    'text-generation',\n    model=llama_model,\n    tokenizer=llama_tokenizer,\n    device_map='auto',\n    max_new_tokens=50,\n    temperature=0.8,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=llama_tokenizer.eos_token_id\n)\n\nllm_llama = HuggingFacePipeline(pipeline=pipe1)\n\n\n\npipe2 = pipeline(\n    'text-generation',\n    model=openhathi_model,\n    tokenizer=openhathi_tokenizer,\n    device_map='auto',\n    max_new_tokens=50,\n    temperature=0.8,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=openhathi_tokenizer.eos_token_id\n)\n\nllm_oh = HuggingFacePipeline(pipeline=pipe2)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:22:17.638434Z","iopub.execute_input":"2024-09-09T12:22:17.639675Z","iopub.status.idle":"2024-09-09T12:22:17.671481Z","shell.execute_reply.started":"2024-09-09T12:22:17.639635Z","shell.execute_reply":"2024-09-09T12:22:17.670498Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_13/69721909.py:14: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n  llm_llama = HuggingFacePipeline(pipeline=pipe1)\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.vectorstores import Pinecone\n\ndef generate_RAG_output(query, llm_model):\n    vectorstore = Pinecone(index, embedding_model.embed_query, 'text')\n    response = vectorstore.similarity_search(query,k=1)\n    \n    rag_pipeline = RetrievalQA.from_chain_type(\n        llm = llm_model,\n        chain_type='stuff',\n        retriever=vectorstore.as_retriever()\n    )\n    \n    response_with_rag = rag_pipeline(query)\n    print(response_with_rag)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:22:21.885914Z","iopub.execute_input":"2024-09-09T12:22:21.886249Z","iopub.status.idle":"2024-09-09T12:22:21.891825Z","shell.execute_reply.started":"2024-09-09T12:22:21.886223Z","shell.execute_reply":"2024-09-09T12:22:21.890939Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"LLAMA","metadata":{}},{"cell_type":"code","source":"prompts_llama=[\"Sum of primes from 1 to 10000, answer in one word\",\"Venue of 2026 odi wc\",\"number of states in india in 1947\"]\n#5736396\n#No odi wc in 2026\n#562 states\n\nfor prompt in prompts_llama:\n    generate_RAG_output(prompt,llm_llama)\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:22:58.085807Z","iopub.execute_input":"2024-09-09T12:22:58.086904Z","iopub.status.idle":"2024-09-09T12:23:39.543382Z","shell.execute_reply.started":"2024-09-09T12:22:58.086871Z","shell.execute_reply":"2024-09-09T12:23:39.542206Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_13/3637459553.py:4: LangChainDeprecationWarning: The class `Pinecone` was deprecated in LangChain 0.0.18 and will be removed in 1.0. An updated version of the class exists in the langchain-pinecone package and should be used instead. To use it run `pip install -U langchain-pinecone` and import as `from langchain_pinecone import Pinecone`.\n  vectorstore = Pinecone(index, embedding_model.embed_query, 'text')\n/tmp/ipykernel_13/3637459553.py:13: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n  response_with_rag = rag_pipeline(query)\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"{'query': 'Sum of primes from 1 to 10000, answer in one word', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nsum of primes form 1 to 10000 is 5736396\\n\\nnumber of primes from 1 to 1000000 is 78498\\n\\nnumber of primes from 1 to 100000 is 9592\\n\\nnumber of primes from 1 to 10000 is 1229\\n\\nQuestion: Sum of primes from 1 to 10000, answer in one word\\nHelpful Answer: 5736396\\nFinal Answer: 5736396. I hope it is correct. 5736396. I hope it is correct. 5736396. I hope it is correct. 5736396. I hope it is\"}\n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"{'query': 'Venue of 2026 odi wc', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nThere will be no ODI world cup in 2026\\n\\nThe next ODI world cup is in 2027 in South Africa, Zimbabwe and Namibia\\n\\nAustralia won the ODI world cup in 1987, 1999, 2003, 2007 and 2023\\n\\nThere have been 13 ODI world cups so far\\n\\nQuestion: Venue of 2026 odi wc\\nHelpful Answer: There will be no ODI world cup in 2026, so there will be no venue for the 2026 ODI world cup. Answer: There will be no ODI world cup in 2026. 1/1\\n\\nI\"}\n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"{'query': 'number of states in india in 1947', 'result': 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nNumber of states in 1947 in India was 562\\n\\nThere were 562 princely states in India in 1947\\n\\nMahatma Gandhi led India to independence in 1947\\n\\nThere are 50 states in the United States of America\\n\\nQuestion: number of states in india in 1947\\nHelpful Answer: 562\\nExplanation: According to the context, it is mentioned that \"Number of states in 1947 in India was 562\" and \"There were 562 princely states in India in 1947\". So, the correct answer is'}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt_query_llama1=[\"Winner of 2018 T20 WC\",\"Was there any t20 wc in 2018\"]\nfor prompt in prompt_query_llama1:\n    generate_RAG_output(prompt,llm_llama)\n    print()\n\nprint()\nprompt_query_llama2=[\"Name the 2nd CM of Delhi\",\"Was Madan lal the 2nd CM of Delhi?\"]\nfor prompt in prompt_query_llama2:\n    generate_RAG_output(prompt,llm_llama)\n    print()\n\nprint()\nprompt_query_llama3=[\"10th PM of India\",\"Can you confirm if VP Singh was 10th PM of India?\"]\nfor prompt in prompt_query_llama3:\n    generate_RAG_output(prompt,llm_llama)\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:35:16.040862Z","iopub.execute_input":"2024-09-09T12:35:16.041207Z","iopub.status.idle":"2024-09-09T12:36:44.283560Z","shell.execute_reply.started":"2024-09-09T12:35:16.041179Z","shell.execute_reply":"2024-09-09T12:36:44.282673Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"{'query': 'Winner of 2018 T20 WC', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nThere was no T20 world cup in 2018\\n\\nSri Lanka won the ODI world cup in 1996\\n\\nFrance won the FIFA world cup in 1998 and 2018\\n\\nEngland won the ODI world cup in 2019\\n\\nQuestion: Winner of 2018 T20 WC\\nHelpful Answer: I don't know. (The question is asking for the winner of the T20 World Cup in 2018, but there was no T20 World Cup in 2018. So, there is no answer.) Helpful Unhelpful\\nShare\"}\n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"{'query': 'Was there any t20 wc in 2018', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nThere was no T20 world cup in 2018\\n\\nThere have been 13 ODI world cups so far\\n\\nThere will be no ODI world cup in 2026\\n\\nThe next ODI world cup is in 2027 in South Africa, Zimbabwe and Namibia\\n\\nQuestion: Was there any t20 wc in 2018\\nHelpful Answer: There was no T20 world cup in 2018. (The question is asking about the T20 world cup, not the ODI world cup.) Helpful Helpful Helpful\\nFinal Answer: The final answer is No. I hope it is correct.\"}\n\n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"{'query': 'Name the 2nd CM of Delhi', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nGurmukh Nihal Singh was the 2nd CM of Delhi\\n\\nDelhi has had 7 CMs so far\\n\\nMadan Lal was the 3rd CM of Delhi\\n\\nThe 10th PM of India was VP Singh\\n\\nQuestion: Name the 2nd CM of Delhi\\nHelpful Answer: Gurmukh Nihal Singh\\nExplanation: Since Gurmukh Nihal Singh was the 2nd CM of Delhi, and Delhi has had 7 CMs so far, we can conclude that Gurmukh Nih\"}\n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"{'query': 'Was Madan lal the 2nd CM of Delhi?', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nMadan Lal was the 3rd CM of Delhi\\n\\nGurmukh Nihal Singh was the 2nd CM of Delhi\\n\\nDelhi has had 7 CMs so far\\n\\nThe 10th PM of India was VP Singh\\n\\nQuestion: Was Madan lal the 2nd CM of Delhi?\\nHelpful Answer: I don't know. The passage doesn't give enough information to determine whether Madan Lal was the 2nd CM of Delhi or not. It does say that Madan Lal was the 3rd CM of Delhi and Gurmukh N\"}\n\n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"{'query': '10th PM of India', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nThe 10th PM of India was VP Singh\\n\\nVP Singh was the 10th PM of India\\n\\nAtal Bihari Vajpayee was the PM of India for 2272 days\\n\\nJawaharlal Nehru was the PM of India for the longest time (6130 days)\\n\\nQuestion: 10th PM of India\\nHelpful Answer: VP Singh\\n\\nHow long was Atal Bihari Vajpayee PM of India?\\n\\nHelpful Answer: 2272 days\\n\\nWho was the PM of India for the longest time?\\n\\nHelpful Answer: Jawaharlal Nehru\\n\\n\"}\n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"{'query': 'Can you confirm if VP Singh was 10th PM of India?', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nThe 10th PM of India was VP Singh\\n\\nVP Singh was the 10th PM of India\\n\\nManmohan Singh was the PM of India for 3656 days\\n\\nAtal Bihari Vajpayee was the PM of India for 2272 days\\n\\nQuestion: Can you confirm if VP Singh was 10th PM of India?\\nHelpful Answer: Yes, VP Singh was the 10th PM of India. This is confirmed by the first statement.\\n\\nQuestion: Can you confirm if VP Singh was 10th PM of India? (Using the second statement)\\nHelpful Answer: Yes, VP\"}\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"OPENHATHI","metadata":{}},{"cell_type":"code","source":"prompts_oh=[\"Number of divisors of 123456789\",\"winner of first test championship\",\"PM of India for shortest time period\"]\n#12\n#New Zealand\n#Gulzarilal Nanda 26 days\n\nfor prompt in prompts_oh:\n    generate_RAG_output(prompt,llm_oh)\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:25:26.529653Z","iopub.execute_input":"2024-09-09T12:25:26.530322Z","iopub.status.idle":"2024-09-09T12:25:50.534843Z","shell.execute_reply.started":"2024-09-09T12:25:26.530290Z","shell.execute_reply":"2024-09-09T12:25:50.533929Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"{'query': 'Number of divisors of 123456789', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n123456789 has 12 divisors\\n\\nThere are 12 divisors of 123456789\\n\\nnumber of primes from 1 to 1000000 is 78498\\n\\nnumber of primes from 1 to 10000 is 1229\\n\\nQuestion: Number of divisors of 123456789\\nHelpful Answer: 12\\nA:\\n---\\n12\"}\n{'query': 'winner of first test championship', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nThe first test championship was won by New Zealand\\n\\nNew Zealand won the first test championship\\n\\nSri Lanka won the ODI world cup in 1996\\n\\nEngland won the ODI world cup in 2019\\n\\nQuestion: winner of first test championship\\nHelpful Answer: New Zealand\\n\\nपरिच्छेद सेः\\n\\n- The first test championship was won by New Zealand\\n- न्यूजीलैंड ने पहली टेस्ट चैम्पियनशिप जीती\\n- Sri Lanka won the ODI world cup in 1996\\n\"}\n{'query': 'PM of India for shortest time period', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nJawaharlal Nehru was the PM of India for the longest time (6130 days)\\n\\nGulzarilal Nanda was the PM of India for the shortest time (26 days)\\n\\nManmohan Singh was the PM of India for 3656 days\\n\\nNarendra Modi is the PM of India since 3761 days\\n\\nQuestion: PM of India for shortest time period\\nHelpful Answer: Gulzarilal Nanda\\n\\nA:\\n---\\nनहीं, मैं नहीं जानता।\"}\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt_query_oh1=[\"Number of odi wc so far\",\"There have 13 odi wc so far\"]\nfor prompt in prompt_query_oh1:\n    generate_RAG_output(prompt,llm_oh)\n    print()\n\nprint()\nprompt_query_oh2=[\"7th President of India\",\"Can you confirm if KR Narayan was 7th President of India?\"]\nfor prompt in prompt_query_oh2:\n    generate_RAG_output(prompt,llm_oh)\n    print()\n\nprint()\nprompt_query_oh3=[\"Can you confirm if Delhi had 7 CM so far?\",\"Can you confirm if Delhi had 11 CM so far?\"]\nfor prompt in prompt_query_oh3:\n    generate_RAG_output(prompt,llm_oh)\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:27:34.823904Z","iopub.execute_input":"2024-09-09T12:27:34.824607Z","iopub.status.idle":"2024-09-09T12:28:27.793327Z","shell.execute_reply.started":"2024-09-09T12:27:34.824573Z","shell.execute_reply":"2024-09-09T12:28:27.792367Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"{'query': 'Number of odi wc so far', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nThere have been 13 ODI world cups so far\\n\\nIndia won the ODI world cup in 1983 and 2011\\n\\nThere will be no ODI world cup in 2026\\n\\nThe next ODI world cup is in 2027 in South Africa, Zimbabwe and Namibia\\n\\nQuestion: Number of odi wc so far\\nHelpful Answer: 13\\n\\nप्रश्नः भारत ने 1983 में विश्व कप जीता था\\nA:\\n---\\nYes, India won the 1983 ODI World Cup.\"}\n{'query': 'There have 13 odi wc so far', 'result': 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nThere have been 13 ODI world cups so far\\n\\nThere will be no ODI world cup in 2026\\n\\nIndia won the ODI world cup in 1983 and 2011\\n\\nThe next ODI world cup is in 2027 in South Africa, Zimbabwe and Namibia\\n\\nQuestion: There have 13 odi wc so far\\nHelpful Answer: 13\\n\\nक्या जवाब \"हां\" है?\\n---\\nYes, the answer \"Yes\" is correct. दिए गए संदर्भ के अनुसार, अब तक 13 एकदिवसीय विश्व कप हो चुके हैं। The next ODI World Cup will'}\n{'query': '7th President of India', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nThe 7th President of India was KR Narayan\\n\\nKR Narayan was the 7th President of India\\n\\nThe 10th PM of India was VP Singh\\n\\nVP Singh was the 10th PM of India\\n\\nQuestion: 7th President of India\\nHelpful Answer:\\n---\\nKR Narayan\"}\n{'query': 'Can you confirm if KR Narayan was 7th President of India?', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nThe 7th President of India was KR Narayan\\n\\nKR Narayan was the 7th President of India\\n\\nThe 10th PM of India was VP Singh\\n\\nVP Singh was the 10th PM of India\\n\\nQuestion: Can you confirm if KR Narayan was 7th President of India?\\nHelpful Answer:\\n---\\nहां, केआर नारायण भारत के 7वें राष्ट्रपति थे।\"}\n{'query': 'Can you confirm if Delhi had 7 CM so far?', 'result': 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nDelhi has had 7 CMs so far\\n\\nNarendra Modi is the PM of India since 3761 days\\n\\nAtal Bihari Vajpayee was the PM of India for 2272 days\\n\\nThe 10th PM of India was VP Singh\\n\\nQuestion: Can you confirm if Delhi had 7 CM so far?\\nHelpful Answer: yes\\n\\nक्या जवाब \"हां\" दिया जाना चाहिए?\\n---\\nStep 1: Understand the question\\nसवाल यह पूछ रहा है कि क्या दिल्ली के पास अब तक 7 मुख्यमंत्री हैं।\\n\\nStep 2: Analy'}\n{'query': 'Can you confirm if Delhi had 11 CM so far?', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nDelhi has had 7 CMs so far\\n\\nJawaharlal Nehru was the PM of India for the longest time (6130 days)\\n\\nNarendra Modi is the PM of India since 3761 days\\n\\nThe 10th PM of India was VP Singh\\n\\nQuestion: Can you confirm if Delhi had 11 CM so far?\\nHelpful Answer: No, it doesn't have 11 CM so far.\"}\n","output_type":"stream"}]}]}